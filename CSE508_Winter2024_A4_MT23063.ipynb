{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8193836,"sourceType":"datasetVersion","datasetId":4852992}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Library**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Download NLTK resources\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:07:39.126794Z","iopub.execute_input":"2024-04-22T20:07:39.127355Z","iopub.status.idle":"2024-04-22T20:07:43.862885Z","shell.execute_reply.started":"2024-04-22T20:07:39.127328Z","shell.execute_reply":"2024-04-22T20:07:43.861798Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Clean and preprocess the ‘Text’ and ‘Summary’ column from the dataset**","metadata":{}},{"cell_type":"code","source":"# Load stopwords\nstop_words = set(stopwords.words('english'))\n\n# Function to clean text\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    # Remove HTML tags\n    text = BeautifulSoup(text, \"html.parser\").get_text()\n    # Lowercase the text\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Remove stopwords and extra spaces\n    text = ' '.join(word for word in text.split() if word not in stop_words)\n    return text\n\n# Read the CSV file\ndf = pd.read_csv('/kaggle/input/cse508-winter2024-a4-data/Reviews.csv')\n\n# Clean 'Text' column\ndf['Text'] = df['Text'].apply(clean_text)\n\n# Clean 'Summary' column\ndf['Summary'] = df['Summary'].apply(clean_text)\n\n# Generate new CSV file with cleaned data\ndf.to_csv('/kaggle/working/cleaned_data.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:07:55.065576Z","iopub.execute_input":"2024-04-22T20:07:55.066423Z","iopub.status.idle":"2024-04-22T20:09:58.632925Z","shell.execute_reply.started":"2024-04-22T20:07:55.066389Z","shell.execute_reply":"2024-04-22T20:09:58.632143Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1268662663.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  text = BeautifulSoup(text, \"html.parser\").get_text()\n/tmp/ipykernel_34/1268662663.py:9: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n  text = BeautifulSoup(text, \"html.parser\").get_text()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Model Training**","metadata":{}},{"cell_type":"markdown","source":"**Installation**","metadata":{}},{"cell_type":"code","source":"pip install rouge","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:10:16.051117Z","iopub.execute_input":"2024-04-22T20:10:16.052040Z","iopub.status.idle":"2024-04-22T20:10:30.532397Z","shell.execute_reply.started":"2024-04-22T20:10:16.052011Z","shell.execute_reply":"2024-04-22T20:10:30.531300Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Library**","metadata":{}},{"cell_type":"code","source":"import torch\nimport random\nimport pandas as pd\nfrom tqdm import tqdm\nfrom rouge import Rouge\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:10:37.359861Z","iopub.execute_input":"2024-04-22T20:10:37.360795Z","iopub.status.idle":"2024-04-22T20:10:48.249927Z","shell.execute_reply.started":"2024-04-22T20:10:37.360758Z","shell.execute_reply":"2024-04-22T20:10:48.249002Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Divide the dataset into training and testing (75:25)**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/cleaned_data.csv').head(10000)\n\nselect_column = df[['Score','Text', 'Summary']]\n\n# Split the dataset into training and testing sets (75:25 ratio)\ntraining_df, testing_df = train_test_split(select_column, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:11:33.408218Z","iopub.execute_input":"2024-04-22T20:11:33.408576Z","iopub.status.idle":"2024-04-22T20:11:36.497371Z","shell.execute_reply.started":"2024-04-22T20:11:33.408549Z","shell.execute_reply":"2024-04-22T20:11:36.496566Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Defining Custom Dataset Class**","metadata":{}},{"cell_type":"code","source":"class CustomData(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length):\n        super().__init__()\n        self.df = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        review_text = str(self.df.iloc[idx]['Text'])\n        summary_text = str(self.df.iloc[idx]['Summary'])\n\n        # Combine review text and summary text\n        text = f\"Review Text: {review_text}\\nSummary: {summary_text}\"\n\n        # Tokenize the combined text\n        inputs = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        input_ids = inputs['input_ids'].squeeze(0)\n        attention_mask = inputs['attention_mask'].squeeze(0)\n\n        # Convert score to tensor\n        label = torch.tensor(self.df.iloc[idx]['Score'])\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'label': label\n        }","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:11:46.870797Z","iopub.execute_input":"2024-04-22T20:11:46.871158Z","iopub.status.idle":"2024-04-22T20:11:46.879993Z","shell.execute_reply.started":"2024-04-22T20:11:46.871129Z","shell.execute_reply":"2024-04-22T20:11:46.879120Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Training Loop**","metadata":{}},{"cell_type":"code","source":"# Instantiate GPT-2 tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Define dataset and dataloader\ntraining_dataset = CustomData(training_df, tokenizer, max_length=128)\ntrain_loader = DataLoader(training_dataset, batch_size=10, shuffle=True)\n\n# Define optimizer and scheduler\nlearning_rate = 1e-5\nepochs = 3\nwarmup_steps = int(0.1 * len(train_loader) * epochs)\noptimizer = AdamW(model.parameters(), lr=learning_rate)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_loader) * epochs)\n\n# Fine-tuning loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.train()\n\nfor epoch in range(epochs):\n    for batch in train_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        # Forward pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n        loss = outputs.loss\n        # Backward pass\n        loss.backward()\n        # Update weights\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"/kaggle/working/fine_tuned_gpt2_Model\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:12:07.749814Z","iopub.execute_input":"2024-04-22T20:12:07.750746Z","iopub.status.idle":"2024-04-22T20:25:14.991488Z","shell.execute_reply.started":"2024-04-22T20:12:07.750704Z","shell.execute_reply":"2024-04-22T20:25:14.990641Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24cb97360e1745849aa469556afd538f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d070b9632a00453e834b3d5b2dcbabc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eba705c2ba347e29ee522b6162ab444"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbb522e2582c4e2ebef5e65b37ce41f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd54e5aceee14f36b7b6672e6e677172"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edde5496806d41f88fcb26a60f08820a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fca1489d6c94e74aecd1e48fd4cd4cd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Loss: 2.988635778427124\nEpoch 2/3, Loss: 1.8884292840957642\nEpoch 3/3, Loss: 1.8239219188690186\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Generating Summaries**","metadata":{}},{"cell_type":"code","source":"def generate_summary(review_text):\n    # Tokenize the review text\n    inputs = tokenizer.encode_plus(\n        review_text,\n        return_tensors=\"pt\",\n        max_length=1024,\n        truncation=True\n    )\n\n    # Move input tensors to the same device as the model\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n\n    # Generate summary using the model\n    model = GPT2LMHeadModel.from_pretrained('/kaggle/working/fine_tuned_gpt2_Model').to(device)\n    summary_ids = model.generate(inputs['input_ids'], max_length=1024, num_beams=4, early_stopping=True)\n\n    # Decode the generated summary tokens\n    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n    return generated_summary","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:10.249619Z","iopub.execute_input":"2024-04-22T20:51:10.250028Z","iopub.status.idle":"2024-04-22T20:51:10.257677Z","shell.execute_reply.started":"2024-04-22T20:51:10.249996Z","shell.execute_reply":"2024-04-22T20:51:10.256571Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"**ROUGE Calculation Function**","metadata":{}},{"cell_type":"code","source":"import csv\nfrom rouge import Rouge\n\n# Function to calculate ROUGE scores\ndef rougescore(generated_summary, actual_summary):\n    rouge = Rouge()\n    rougescore = rouge.get_scores(generated_summary, actual_summary)\n    return rougescore","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:18.407235Z","iopub.execute_input":"2024-04-22T20:51:18.407619Z","iopub.status.idle":"2024-04-22T20:51:18.413124Z","shell.execute_reply.started":"2024-04-22T20:51:18.407580Z","shell.execute_reply":"2024-04-22T20:51:18.412119Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"**Save CSV File**","metadata":{}},{"cell_type":"code","source":"# Read CSV file\ncsv_file = '/kaggle/working/cleaned_data.csv'\noutput_file = '/kaggle/working/rougescore.csv'  \nnum_rows = 200 \n\n# Open output file in write mode\nwith open(output_file, mode='w', newline='', encoding='utf-8') as output_csv:\n    csv_writer = csv.writer(output_csv)\n    csv_writer.writerow(['Text', 'Generated Summary', 'ROUGE-1 Precision', 'ROUGE-1 Recall', 'ROUGE-1 F1',\n                         'ROUGE-2 Precision', 'ROUGE-2 Recall', 'ROUGE-2 F1',\n                         'ROUGE-L Precision', 'ROUGE-L Recall', 'ROUGE-L F1'])\n    with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n        csv_reader = csv.DictReader(file)\n        for idx, row in enumerate(csv_reader):\n            if idx >= num_rows:\n                break  \n\n            review_text = row['Text']\n            actual_summary = row['Summary']  # Adjust column name\n\n            # Skip rows with empty actual summary\n            if not actual_summary:\n                print(f\"Skipping row {idx + 1} due to empty Summary.\")\n                continue\n\n            # Generate summary\n            gen_summary = generate_summary(review_text)\n            splited_summary = gen_summary.split(review_text)\n            generated_summary = splited_summary[1].strip()\n\n            # Calculate ROUGE scores\n            rouge_scores = rougescore(generated_summary, actual_summary)\n\n            # Write results to output file\n            csv_writer.writerow([review_text, generated_summary,\n                                 rouge_scores[0]['rouge-1']['p'], rouge_scores[0]['rouge-1']['r'], rouge_scores[0]['rouge-1']['f'],\n                                 rouge_scores[0]['rouge-2']['p'], rouge_scores[0]['rouge-2']['r'], rouge_scores[0]['rouge-2']['f'],\n                                 rouge_scores[0]['rouge-l']['p'], rouge_scores[0]['rouge-l']['r'], rouge_scores[0]['rouge-l']['f']])\n\n    print(\"ROUGE scores calculated and CSV File saved to\", output_file)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:01:07.426947Z","iopub.execute_input":"2024-04-22T21:01:07.427652Z","iopub.status.idle":"2024-04-22T21:01:07.439717Z","shell.execute_reply.started":"2024-04-22T21:01:07.427617Z","shell.execute_reply":"2024-04-22T21:01:07.438723Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"ROUGE scores calculated and CSV File saved to /kaggle/working/rougescore.csv\n","output_type":"stream"}]}]}